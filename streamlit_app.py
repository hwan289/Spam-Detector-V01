import os
import sys
import time
import streamlit as st
import pandas as pd
import io
import numpy as np 

# --- CONFIGURATION: Intelligent Threading & Resource Limits ---
# 1. Mac (Darwin): Strict single-threading needed to prevent "mutex" crashes.
# 2. Streamlit Cloud (Linux): We limit threads to 2 to prevent "Out of Memory" (OOM) kills 
#    and CPU thrashing on Free Tier (which usually has ~3GB RAM limits).
if sys.platform == "darwin":
    os.environ["OMP_NUM_THREADS"] = "1"
    os.environ["MKL_NUM_THREADS"] = "1"
    os.environ["OPENBLAS_NUM_THREADS"] = "1"
    os.environ["VECLIB_MAXIMUM_THREADS"] = "1"
    os.environ["NUMEXPR_NUM_THREADS"] = "1"
    os.environ["TOKENIZERS_PARALLELISM"] = "false"
else:
    # Linux / Cloud settings (Memory Safety)
    os.environ["OMP_NUM_THREADS"] = "2"
    os.environ["MKL_NUM_THREADS"] = "2"
    os.environ["OPENBLAS_NUM_THREADS"] = "2"
    os.environ["NUMEXPR_NUM_THREADS"] = "2"
    # Allow tokenizers parallelism but keep it modest
    os.environ["TOKENIZERS_PARALLELISM"] = "true"

# -----------------------------------------------------------------------------
# Page Configuration
# -----------------------------------------------------------------------------
st.set_page_config(
    page_title="BERTopic Explorer",
    page_icon="ðŸ§ ",
    layout="wide",
    initial_sidebar_state="expanded"
)

# -----------------------------------------------------------------------------
# Check Environment (Debug Help)
# -----------------------------------------------------------------------------
if sys.platform == "darwin":
    if os.environ.get('OBJC_DISABLE_INITIALIZE_FORK_SAFETY') != 'YES':
        st.error("""
        ðŸ›‘ **Critical Error: App launched incorrectly (Local Mac)**
        
        You are running on macOS, which requires a specific security flag to be disabled.
        Please stop this app and run it using the `run_app.sh` script provided.
        """)
        st.stop()

# -----------------------------------------------------------------------------
# Language & Translations
# -----------------------------------------------------------------------------
if 'lang' not in st.session_state:
    st.session_state['lang'] = 'en'

def toggle_language():
    st.session_state['lang'] = 'zh' if st.session_state['lang'] == 'en' else 'en'

TRANS = {
    'title': { 'en': "ðŸ§  BERTopic Interactive Explorer", 'zh': "ðŸ§  BERTopic äº¤äº’å¼æŽ¢ç´¢å™¨" },
    'desc': {'en': "Advanced Topic Modeling with BERTopic.", 'zh': "BERTopic é«˜çº§ä¸»é¢˜å»ºæ¨¡ã€‚"},
    'sidebar_config': {'en': "Configuration", 'zh': "é…ç½®"},
    'remove_stopwords': {'en': "Remove Stopwords (English)", 'zh': "ç§»é™¤åœç”¨è¯ (è‹±æ–‡)"},
    'lemmatize': {'en': "Combine Variations (Lemmatize)", 'zh': "åˆå¹¶è¯å½¢å˜ä½“ (Lemmatize)"},
    'lemmatize_help': {'en': "Converts words to base form (e.g., 'students' -> 'student'). Slower but cleaner.", 'zh': "å°†å•è¯è½¬æ¢ä¸ºåŸºæœ¬å½¢å¼ï¼ˆä¾‹å¦‚ 'students' -> 'student'ï¼‰ã€‚é€Ÿåº¦è¾ƒæ…¢ä½†ç»“æžœæ›´æ•´æ´ã€‚"},
    'data_loading': {'en': "Data Loading", 'zh': "æ•°æ®åŠ è½½"},
    'upload_csv': {'en': "Upload CSV", 'zh': "ä¸Šä¼  CSV"},
    'train_btn': {'en': "ðŸš€ Train BERTopic Model", 'zh': "ðŸš€ è®­ç»ƒ BERTopic æ¨¡åž‹"},
    'status_start': {'en': "Starting Process...", 'zh': "æ­£åœ¨å¯åŠ¨æµç¨‹..."},
    'step_1': {'en': "âš™ï¸ [1/3] Loading AI Model...", 'zh': "âš™ï¸ [1/3] åŠ è½½ AI æ¨¡åž‹..."},
    'step_2': {'en': "ðŸƒ [2/3] Embedding & Clustering (This takes the longest)...", 'zh': "ðŸƒ [2/3] åµŒå…¥ä¸Žèšç±» (è¿™ä¸€æ­¥æœ€è€—æ—¶)..."},
    'train_complete': {'en': "Complete! Time: {:.2f}s", 'zh': "å®Œæˆ! è€—æ—¶: {:.2f} ç§’"},
    'results_header': {'en': "Results Analysis", 'zh': "ç»“æžœåˆ†æž"},
    'upload_prompt': {'en': "Please upload a CSV file to begin.", 'zh': "è¯·ä¸Šä¼  CSV æ–‡ä»¶ä»¥å¼€å§‹ã€‚"},
    'no_topics_warning': {
        'en': "âš ï¸ No topics were found! Everything was classified as outliers (-1). Try decreasing 'Min Topic Size' or adding more data.",
        'zh': "âš ï¸ æœªå‘çŽ°ä»»ä½•ä¸»é¢˜ï¼æ‰€æœ‰å†…å®¹éƒ½è¢«å½’ç±»ä¸ºç¦»ç¾¤å€¼ (-1)ã€‚è¯·å°è¯•å‡å°â€œæœ€å°ä¸»é¢˜å¤§å°â€æˆ–æ·»åŠ æ›´å¤šæ•°æ®ã€‚"
    },
    'viz_error': {'en': "Visualization not available: {}", 'zh': "æ— æ³•ç”Ÿæˆå¯è§†åŒ–: {}"},
    'help_info_title': {'en': "â„¹ï¸ How to interpret this table", 'zh': "â„¹ï¸ å¦‚ä½•è§£è¯»æ­¤è¡¨"},
    'help_info_text': {
        'en': "**Topic:** The ID of the topic. -1 refers to 'outliers' (noise).\n**Count:** Documents in this topic.\n**Name:** Keywords representing the topic.",
        'zh': "**Topic:** ä¸»é¢˜ IDã€‚-1 ä»£è¡¨â€œç¦»ç¾¤å€¼â€ï¼ˆå™ªéŸ³ï¼‰ã€‚\n**Count:** æ–‡æ¡£æ•°é‡ã€‚\n**Name:** ä»£è¡¨è¯¥ä¸»é¢˜çš„å…³é”®è¯ã€‚"
    },
    'help_dist_title': {'en': "â„¹ï¸ How to interpret the Distance Map", 'zh': "â„¹ï¸ å¦‚ä½•è§£è¯»è·ç¦»å›¾"},
    'help_dist_text': {
        'en': "**Circles:** Topics.\n**Distance:** Closer circles = Similar meanings.",
        'zh': "**åœ†åœˆ:** ä¸»é¢˜ã€‚\n**è·ç¦»:** åœ†åœˆè¶Šè¿‘ = å«ä¹‰è¶Šç›¸ä¼¼ã€‚"
    },
    'help_bar_title': {'en': "â„¹ï¸ How to interpret the Bar Chart", 'zh': "â„¹ï¸ å¦‚ä½•è§£è¯»æ¡å½¢å›¾"},
    'help_bar_text': {
        'en': "Shows distinct keywords for each topic based on c-TF-IDF score.",
        'zh': "åŸºäºŽ c-TF-IDF åˆ†æ•°æ˜¾ç¤ºæ¯ä¸ªä¸»é¢˜çš„ç‹¬ç‰¹å…³é”®è¯ã€‚"
    },
    'help_heat_title': {'en': "â„¹ï¸ How to interpret the Similarity Heatmap", 'zh': "â„¹ï¸ å¦‚ä½•è§£è¯»ç›¸ä¼¼åº¦çƒ­åŠ›å›¾"},
    'help_heat_text': {
        'en': "Shows similarity between topics. Dark blue = High similarity.",
        'zh': "æ˜¾ç¤ºä¸»é¢˜é—´çš„ç›¸ä¼¼åº¦ã€‚æ·±è“è‰² = é«˜ç›¸ä¼¼åº¦ã€‚"
    }
}

def t(key):
    return TRANS.get(key, {}).get(st.session_state['lang'], "Missing")

# -----------------------------------------------------------------------------
# Styling Helpers
# -----------------------------------------------------------------------------
def style_fig(fig):
    if fig:
        fig.update_layout(hoverlabel=dict(bgcolor="#333333", font_color="#4b8bf5", font_family="sans-serif", bordercolor="#4b8bf5"))
    return fig

# -----------------------------------------------------------------------------
# Caching High-Cost Resources (Speed Boost)
# -----------------------------------------------------------------------------
@st.cache_resource
def load_embedding_model():
    """Loads the sentence transformer model once and keeps it in memory."""
    from sentence_transformers import SentenceTransformer
    # 'all-MiniLM-L6-v2' is the fastest accurate model for CPU
    return SentenceTransformer('all-MiniLM-L6-v2')

# -----------------------------------------------------------------------------
# Sidebar
# -----------------------------------------------------------------------------
st.sidebar.button("ðŸŒ English / ä¸­æ–‡", on_click=toggle_language)
st.sidebar.title(t('sidebar_config'))

remove_stopwords = st.sidebar.checkbox(t('remove_stopwords'), value=True)
use_lemmatization = st.sidebar.checkbox(t('lemmatize'), value=False)

# FAST MODE TOGGLE
fast_mode = st.sidebar.checkbox(t('fast_mode'), value=True, help=t('fast_mode_help'))

docs = []
uploaded_file = st.sidebar.file_uploader(t('upload_csv'), type=["csv"])

if uploaded_file:
    try:
        df = pd.read_csv(uploaded_file)
        text_col = st.sidebar.selectbox("Text Column", df.columns)
        
        # Data Cleaning
        df = df.dropna(subset=[text_col])
        df = df[df[text_col].astype(str).str.strip() != '']
        df = df.reset_index(drop=True)
        
        if len(df) == 0:
            st.error("Error: No valid text data found.")
        else:
            # SAMPLING LOGIC
            if fast_mode and len(df) > 5000:
                df = df.sample(5000, random_state=42)
                st.sidebar.warning(f"âš¡ Fast Mode: Using 5,000 random samples out of {len(uploaded_file.getvalue())} bytes.")
            
            docs = df[text_col].astype(str).tolist()
            st.sidebar.success(f"Loaded {len(docs)} docs")
    except Exception as e:
        st.sidebar.error(f"Error reading CSV: {e}")

# -----------------------------------------------------------------------------
# Helpers
# -----------------------------------------------------------------------------
def get_lemmatizer_analyzer():
    from sklearn.feature_extraction.text import CountVectorizer
    try:
        import nltk
        from nltk.stem import WordNetLemmatizer
        try:
            nltk.data.find('corpora/wordnet')
            nltk.data.find('corpora/omw-1.4')
        except LookupError:
            with st.spinner("Downloading NLTK data (WordNet)... this happens once."):
                nltk.download('wordnet')
                nltk.download('omw-1.4')
    except ImportError:
        return None  
    lemmatizer = WordNetLemmatizer()
    analyzer = CountVectorizer(stop_words="english").build_analyzer()
    def lemmatized_words(doc):
        return [lemmatizer.lemmatize(w) for w in analyzer(doc)]
    return lemmatized_words

# -----------------------------------------------------------------------------
# Main App Logic
# -----------------------------------------------------------------------------
st.title(t('title'))

# Model Params
language = st.sidebar.selectbox("Language", ["english", "multilingual"], index=0)

st.sidebar.markdown("---")
st.sidebar.markdown("**Step 1: Discovery**")
min_topic_size = st.sidebar.number_input("Min Topic Size", min_value=2, value=5, step=1)

st.sidebar.markdown("**Step 2: Reduction**")
auto_topics = st.sidebar.checkbox("Auto Reduce Topics", value=True)
if auto_topics:
    nr_topics = "auto"
else:
    nr_topics = st.sidebar.slider("Target Max Topics", 5, 300, 20)

st.sidebar.markdown("---")
auto_adjust_params = st.sidebar.checkbox("Auto-adjust parameters for small data", value=True)

if st.button(t('train_btn'), type="primary", disabled=(not docs)):
    start_time = time.time()
    with st.status(t('status_start'), expanded=True) as status:
        try:
            # 1. Load Embedding Model (Cached)
            st.write(t('step_1'))
            import torch
            # Only set threads if on Mac
            if sys.platform == "darwin":
                torch.set_num_threads(1)
            
            from bertopic import BERTopic
            from umap import UMAP
            from hdbscan import HDBSCAN
            from sklearn.feature_extraction.text import CountVectorizer 

            # Load Cached Model
            embedding_model = load_embedding_model()

            # 2. Configure Vectorizer
            if use_lemmatization:
                custom_analyzer = get_lemmatizer_analyzer()
                if custom_analyzer:
                    vectorizer_model = CountVectorizer(analyzer=custom_analyzer)
                else:
                    st.warning("NLTK missing, skipping lemmatization.")
                    vectorizer_model = CountVectorizer(stop_words="english") if remove_stopwords else None
            else:
                vectorizer_model = CountVectorizer(stop_words="english") if remove_stopwords else None

            # 3. Configure Sub-models
            n_samples = len(docs)
            
            # Safety checks
            safe_min_topic_size = min_topic_size
            if min_topic_size >= n_samples:
                safe_min_topic_size = max(2, n_samples - 1)

            # UMAP Settings
            n_neighbors_val = 15
            n_components_val = 5
            
            if auto_adjust_params and n_samples < 20:
                n_neighbors_val = max(2, min(15, n_samples - 1))
                n_components_val = max(2, min(5, n_samples - 2))
            
            umap_model = UMAP(
                n_neighbors=n_neighbors_val, 
                n_components=n_components_val, 
                min_dist=0.0, 
                metric='cosine', 
                low_memory=True, # Optimized for Streamlit Cloud Memory Limits
                n_jobs=1 
            )
            
            hdbscan_model = HDBSCAN(
                min_cluster_size=safe_min_topic_size, 
                metric='euclidean', 
                cluster_selection_method='eom', 
                prediction_data=True, 
                core_dist_n_jobs=1
            )

            topic_model = BERTopic(
                language=language,
                nr_topics=nr_topics if nr_topics == "auto" else int(nr_topics),
                min_topic_size=safe_min_topic_size,
                vectorizer_model=vectorizer_model,
                umap_model=umap_model,
                hdbscan_model=hdbscan_model,
                embedding_model=embedding_model, # Use cached model
                verbose=True
            )

            # 4. Fit
            st.write(t('step_2'))
            
            clean_docs = [str(d) for d in docs]
            
            try:
                topics, probs = topic_model.fit_transform(clean_docs)
            except ValueError as ve:
                st.error(f"Analysis failed: {ve}")
                st.stop()
            
            # Flatten & Check
            topics_list = np.array(topics).flatten().tolist()
            topics_list = [int(t) for t in topics_list]
            
            if len(clean_docs) != len(topics_list):
                min_len = min(len(clean_docs), len(topics_list))
                clean_docs = clean_docs[:min_len]
                topics_list = topics_list[:min_len]

            # 5. Update Keywords
            try:
                topic_model.update_topics(clean_docs, topics_list, vectorizer_model=vectorizer_model)
            except Exception as e:
                st.warning(f"Keyword fine-tuning skipped: {e}")

            # Store
            st.session_state['model'] = topic_model
            st.session_state['docs'] = clean_docs
            st.session_state['topics'] = topics_list
            
            st.success(t('train_complete').format(time.time() - start_time))
            status.update(label="Done", state="complete", expanded=False)

        except Exception as e:
            st.error(f"Error: {str(e)}")

# -----------------------------------------------------------------------------
# Visualization Section
# -----------------------------------------------------------------------------
if 'model' in st.session_state:
    model = st.session_state['model']
    topic_info = model.get_topic_info()
    real_topic_count = len(topic_info) - 1 
    has_topics = real_topic_count > 0
    
    st.divider()
    st.header(t('results_header'))
    
    import plotly.express as px

    tab1, tab2, tab3, tab4 = st.tabs(["Topic Info", "Distance Map", "Bar Chart", "Heatmap"])
    
    with tab1:
        with st.expander(t('help_info_title')): st.markdown(t('help_info_text'))
        st.dataframe(topic_info, use_container_width=True)
        t_ids = topic_info['Topic'].values
        sel_t = st.selectbox("Explore Topic", t_ids)
        if sel_t is not None: st.write(model.get_topic(sel_t))

    with tab2:
        with st.expander(t('help_dist_title')): st.markdown(t('help_dist_text'))
        if not has_topics: st.warning(t('no_topics_warning'))
        elif real_topic_count < 4: st.info("Not enough topics for Distance Map.")
        else:
            try:
                fig = model.visualize_topics()
                st.plotly_chart(style_fig(fig), use_container_width=True)
            except Exception as e: st.warning(t('viz_error').format(e))

    with tab3:
        with st.expander(t('help_bar_title')): st.markdown(t('help_bar_text'))
        if not has_topics: st.warning(t('no_topics_warning'))
        else:
            try:
                fig = model.visualize_barchart(top_n_topics=10)
                st.plotly_chart(style_fig(fig), use_container_width=True)
            except Exception as e: st.warning(t('viz_error').format(e))

    with tab4:
        with st.expander(t('help_heat_title')): st.markdown(t('help_heat_text'))
        if not has_topics: st.warning(t('no_topics_warning'))
        else:
            try:
                fig = model.visualize_heatmap()
                st.plotly_chart(style_fig(fig), use_container_width=True)
            except Exception as e: st.warning(t('viz_error').format(e))
elif not docs:
    st.info(t('upload_prompt'))
